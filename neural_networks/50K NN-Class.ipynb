{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest — more or less than 50K/year\n",
    "Now with optimized random forests and neural networks!\n",
    "\n",
    "## Step 0: Importing\n",
    "First, we import the libraries needed to read the data, to use a `RandomForestClassifier`, a `LabelEncoder`, `GridSearchCV`, and our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats as stats\n",
    "import keras\n",
    "from keras.datasets import mnist, reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, results):\n",
    "    corrects = predictions == results\n",
    "    num_correct = sum(corrects)\n",
    "    return num_correct/len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.5: Data parsing\n",
    "Then, we parse the data from `adult_data.csv` (from https://archive.ics.uci.edu/ml/datasets/Adult) and we clarify which columns should be used as training data — we exclude fnlwgt and the result, as those both relate to income, which is what our output is (so we don't want them as an input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/adult_data.csv\")\n",
    "df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = ['age', 'workclass', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   education  education-num       marital-status  \\\n",
       "0   50   Self-emp-not-inc   Bachelors             13   Married-civ-spouse   \n",
       "1   38            Private     HS-grad              9             Divorced   \n",
       "2   53            Private        11th              7   Married-civ-spouse   \n",
       "3   28            Private   Bachelors             13   Married-civ-spouse   \n",
       "4   37            Private     Masters             14   Married-civ-spouse   \n",
       "\n",
       "           occupation    relationship    race      sex  capital-gain  \\\n",
       "0     Exec-managerial         Husband   White     Male             0   \n",
       "1   Handlers-cleaners   Not-in-family   White     Male             0   \n",
       "2   Handlers-cleaners         Husband   Black     Male             0   \n",
       "3      Prof-specialty            Wife   Black   Female             0   \n",
       "4     Exec-managerial            Wife   White   Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week  native-country  \n",
       "0             0              13   United-States  \n",
       "1             0              40   United-States  \n",
       "2             0              40   United-States  \n",
       "3             0              40            Cuba  \n",
       "4             0              40   United-States  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[test_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32560, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set aside some data for training with and some for testing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_training = int(0.8*df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training = df.iloc[:num_training, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26048, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing = df.iloc[num_training:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_transformed = pd.DataFrame()\n",
    "for feature in all_features:\n",
    "    df_training_transformed[feature] = le.fit_transform(df_training[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_testing_transformed = pd.DataFrame()\n",
    "for feature in all_features:\n",
    "    df_testing_transformed[feature] = le.fit_transform(df_testing[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the X input data and the Y output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = df_training_transformed[test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_training_transformed['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Training our original, random forest model\n",
    "We then create a `RandomForestClassifier()` which we train with out training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use `clf.predict` and compare the predict results of our test data with the actual results to find the accuracy of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(df_testing_transformed[test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_perc = accuracy(clf.predict(df_testing_transformed[test_features]), df_testing_transformed['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training our optimized random forest model using GridSearchCV()\n",
    "Using `GridSearchCV()`, we can optimize the parameters of our model. First, we create a new `RandomForestClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimized_forest = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the parameters that we're adjusting for and what values they can take on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"n_estimators\": [110, 120], \"min_samples_split\": [25, 30, 35]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a GridSearchCV optimized model and fit it with our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_optimized = GridSearchCV(optimized_forest, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [110, 120], 'min_samples_split': [25, 30, 35]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_optimized.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the `best_estimator` of our data (AKA the values that were chosen by `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=25,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=120, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_optimized.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our original model, we can test our model with new data to find its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_optimized.predict(df_testing_transformed[test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_optimized_perc = accuracy(clf_optimized.predict(df_testing_transformed[test_features]), df_testing_transformed['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 0.808814496314\n",
      "Optimized: 0.820792383292\n"
     ]
    }
   ],
   "source": [
    "print(\"Original: \" + str(clf_perc))\n",
    "print(\"Optimized: \" + str(clf_optimized_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Voila! We see that our `RandomForestClassifier`, when optimized with `GridSearchCV`, is more accurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: A Neural Network for classification\n",
    "Based off the work of Carl Shan and the Keras examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048 train sequences\n",
      "6512 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(df_testing_transformed['result']), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(y_train) + 1 # How many classes we have.\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = df_testing_transformed[test_features] # Only get the features for testing in our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make x_test processable by Keras.\n",
    "x_test = x_test.values\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts data to a binary matrix, which allows for categorization\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(df_testing_transformed['result'], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # The batch size, set to save memory\n",
    "epochs = 20 # The number of epochs for which we want to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.values # Make the data processable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we train the model. I chose the layer hyperparameters based on the recommendations here: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw, specifically the answer of `hobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the optimizer `adadelta` based on seeing someone else use it here: https://datascience.stackexchange.com/questions/10048/what-is-the-best-keras-model-for-multi-class-classification & based on seeing this image in class: https://cdn-images-1.medium.com/max/1600/1*SjtKOauOXFVjWRR7iCtHiA.gif. When it ended up working, I was more than happy to keep using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 896)               12544     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 896)               803712    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1794      \n",
      "=================================================================\n",
      "Total params: 818,050\n",
      "Trainable params: 818,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 26048 samples, validate on 6512 samples\n",
      "Epoch 1/20\n",
      "26048/26048 [==============================] - 21s 817us/step - loss: 0.5207 - acc: 0.7882 - val_loss: 0.3989 - val_acc: 0.8285\n",
      "Epoch 2/20\n",
      "26048/26048 [==============================] - 21s 793us/step - loss: 0.4004 - acc: 0.8181 - val_loss: 0.3919 - val_acc: 0.8263\n",
      "Epoch 3/20\n",
      "26048/26048 [==============================] - 18s 701us/step - loss: 0.3861 - acc: 0.8247 - val_loss: 0.3643 - val_acc: 0.8331\n",
      "Epoch 4/20\n",
      "26048/26048 [==============================] - 17s 651us/step - loss: 0.3760 - acc: 0.8302 - val_loss: 0.3551 - val_acc: 0.8366\n",
      "Epoch 5/20\n",
      "26048/26048 [==============================] - 19s 716us/step - loss: 0.3646 - acc: 0.8332 - val_loss: 0.3622 - val_acc: 0.8349\n",
      "Epoch 6/20\n",
      "26048/26048 [==============================] - 22s 861us/step - loss: 0.3517 - acc: 0.8373 - val_loss: 0.3546 - val_acc: 0.8280\n",
      "Epoch 7/20\n",
      "26048/26048 [==============================] - 19s 748us/step - loss: 0.3464 - acc: 0.8386 - val_loss: 0.3613 - val_acc: 0.8268\n",
      "Epoch 8/20\n",
      "26048/26048 [==============================] - 20s 752us/step - loss: 0.3417 - acc: 0.8411 - val_loss: 0.3505 - val_acc: 0.8323\n",
      "Epoch 9/20\n",
      "26048/26048 [==============================] - 20s 786us/step - loss: 0.3400 - acc: 0.8409 - val_loss: 0.3374 - val_acc: 0.8452\n",
      "Epoch 10/20\n",
      "26048/26048 [==============================] - 20s 783us/step - loss: 0.3380 - acc: 0.8416 - val_loss: 0.3550 - val_acc: 0.8337\n",
      "Epoch 11/20\n",
      "26048/26048 [==============================] - 20s 783us/step - loss: 0.3359 - acc: 0.8435 - val_loss: 0.3619 - val_acc: 0.8320\n",
      "Epoch 12/20\n",
      "26048/26048 [==============================] - 21s 822us/step - loss: 0.3370 - acc: 0.8438 - val_loss: 0.3498 - val_acc: 0.8369\n",
      "Epoch 13/20\n",
      "26048/26048 [==============================] - 19s 728us/step - loss: 0.3343 - acc: 0.8456 - val_loss: 0.3713 - val_acc: 0.8226\n",
      "Epoch 14/20\n",
      "26048/26048 [==============================] - 20s 749us/step - loss: 0.3322 - acc: 0.8471 - val_loss: 0.3539 - val_acc: 0.8369\n",
      "Epoch 15/20\n",
      "26048/26048 [==============================] - 19s 724us/step - loss: 0.3338 - acc: 0.8448 - val_loss: 0.3392 - val_acc: 0.8415\n",
      "Epoch 16/20\n",
      "26048/26048 [==============================] - 21s 790us/step - loss: 0.3331 - acc: 0.8452 - val_loss: 0.3366 - val_acc: 0.8447\n",
      "Epoch 17/20\n",
      "26048/26048 [==============================] - 16s 620us/step - loss: 0.3308 - acc: 0.8446 - val_loss: 0.3509 - val_acc: 0.8371\n",
      "Epoch 18/20\n",
      "26048/26048 [==============================] - 17s 638us/step - loss: 0.3311 - acc: 0.8442 - val_loss: 0.3529 - val_acc: 0.8317\n",
      "Epoch 19/20\n",
      "26048/26048 [==============================] - 19s 742us/step - loss: 0.3315 - acc: 0.8457 - val_loss: 0.3471 - val_acc: 0.8352\n",
      "Epoch 20/20\n",
      "26048/26048 [==============================] - 21s 817us/step - loss: 0.3308 - acc: 0.8455 - val_loss: 0.3448 - val_acc: 0.8403\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential() # Define our model as a sequential neural net\n",
    "model.add(Dense(896, activation='relu', input_shape=(13,))) # Add a 13 node input layer\n",
    "model.add(Dropout(0.2)) # Set dropout to .2 to reduce the total number of connections to prevent overfitting.\n",
    "model.add(Dense(896, activation='relu')) # The a 896 node hidden layer.\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax')) # set a softmax final layer for our output.\n",
    "\n",
    "model.summary()\n",
    "# opt = SGD(lr=0.01)\n",
    "model.compile(optimizer='adadelta', # Using adadelta based on https://cdn-images-1.medium.com/max/1600/1*SjtKOauOXFVjWRR7iCtHiA.gif\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit our model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.344766228659\n",
      "Test accuracy: 0.840294840295\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "neural_net_perc = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Random Forest: 0.808814496314\n",
      "Optimized Random Forest: 0.820792383292\n",
      "20-Epoch Neural Network: 0.840294840295\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Random Forest: \" + str(clf_perc))\n",
    "print(\"Optimized Random Forest: \" + str(clf_optimized_perc))\n",
    "print(str(epochs) + \"-Epoch Neural Network: \" + str(neural_net_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our neural network is the most accurate of the 3 models. I read the following article: http://blog.citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics, and found that the tradeoffs between random forests & neural networks are very data-dependent. For our case neural networks are more effective mainly because they can find unclear, complicated relationships within classification that may not be apparent, they delve into a level of complexity not seem by random forests, and they don't overfit. That's a big one. Although I increased the `min_samples_split` within the grid-searched classifier, overfitting is still possible because of the noise and variance that comes with determining income. With neural networks, we can set a `Dropout` so that not all connections are made (while keeping the model accurate)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
